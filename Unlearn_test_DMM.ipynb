{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nem-42098/SL_Proj_Unlearning/blob/main/Unlearn_test_DMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTso4SFgePeq"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nem-42098/SL_Proj_Unlearning/blob/main/Unlearn_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCIXLmbpKuwo",
        "outputId": "2afa0c37-85c4-4e6f-8c85-467a40d75f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SL_Proj_Unlearning'...\n",
            "remote: Enumerating objects: 183, done.\u001b[K\n",
            "remote: Counting objects: 100% (183/183), done.\u001b[K\n",
            "remote: Compressing objects: 100% (129/129), done.\u001b[K\n",
            "remote: Total 183 (delta 107), reused 120 (delta 51), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (183/183), 47.47 KiB | 1.25 MiB/s, done.\n",
            "Resolving deltas: 100% (107/107), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/nem-42098/SL_Proj_Unlearning.git\n",
        "import os\n",
        "os.chdir('/content/SL_Proj_Unlearning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6CbLaIs8Kuwq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyJgdsOfKuwr"
      },
      "source": [
        "### Load Pre-Trained VGG network\n",
        "> #### https://github.com/chenyaofo\n",
        "> ### Note: There is some issue with using Batch Norm before ReLu as it creates a bias in the network. So people exchange the order between the two for tackling the bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wvUMz6DKuwr",
        "outputId": "667b6eb3-6c77-48a5-bb35-bca571c0cd4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/chenyaofo/pytorch-cifar-models/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "Downloading: \"https://github.com/chenyaofo/pytorch-cifar-models/releases/download/vgg/cifar10_vgg11_bn-eaeebf42.pt\" to /root/.cache/torch/hub/checkpoints/cifar10_vgg11_bn-eaeebf42.pt\n",
            "100%|██████████| 37.3M/37.3M [00:00<00:00, 72.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "### First time when you wan to download the model\n",
        "device=torch.device('cuda')\n",
        "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_vgg11_bn\", pretrained=True)\n",
        "model=model.to(device)\n",
        "### For future uses:Loading from the local\n",
        "\n",
        "# model_1=torch.hub.load(\"C:/Users/nmura/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\",'hubconf.py',source='local')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqWzDJ7YKuwr"
      },
      "source": [
        "### Check which pre-trained model are available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSxUWBRyKuwr",
        "outputId": "437c2e38-1c35-428c-bc04-a0c4e109a483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/chenyaofo/pytorch-cifar-models/zipball/master\" to /root/.cache/torch/hub/master.zip\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cifar100_mobilenetv2_x0_5',\n",
              " 'cifar100_mobilenetv2_x0_75',\n",
              " 'cifar100_mobilenetv2_x1_0',\n",
              " 'cifar100_mobilenetv2_x1_4',\n",
              " 'cifar100_repvgg_a0',\n",
              " 'cifar100_repvgg_a1',\n",
              " 'cifar100_repvgg_a2',\n",
              " 'cifar100_resnet20',\n",
              " 'cifar100_resnet32',\n",
              " 'cifar100_resnet44',\n",
              " 'cifar100_resnet56',\n",
              " 'cifar100_shufflenetv2_x0_5',\n",
              " 'cifar100_shufflenetv2_x1_0',\n",
              " 'cifar100_shufflenetv2_x1_5',\n",
              " 'cifar100_shufflenetv2_x2_0',\n",
              " 'cifar100_vgg11_bn',\n",
              " 'cifar100_vgg13_bn',\n",
              " 'cifar100_vgg16_bn',\n",
              " 'cifar100_vgg19_bn',\n",
              " 'cifar100_vit_b16',\n",
              " 'cifar100_vit_b32',\n",
              " 'cifar100_vit_h14',\n",
              " 'cifar100_vit_l16',\n",
              " 'cifar100_vit_l32',\n",
              " 'cifar10_mobilenetv2_x0_5',\n",
              " 'cifar10_mobilenetv2_x0_75',\n",
              " 'cifar10_mobilenetv2_x1_0',\n",
              " 'cifar10_mobilenetv2_x1_4',\n",
              " 'cifar10_repvgg_a0',\n",
              " 'cifar10_repvgg_a1',\n",
              " 'cifar10_repvgg_a2',\n",
              " 'cifar10_resnet20',\n",
              " 'cifar10_resnet32',\n",
              " 'cifar10_resnet44',\n",
              " 'cifar10_resnet56',\n",
              " 'cifar10_shufflenetv2_x0_5',\n",
              " 'cifar10_shufflenetv2_x1_0',\n",
              " 'cifar10_shufflenetv2_x1_5',\n",
              " 'cifar10_shufflenetv2_x2_0',\n",
              " 'cifar10_vgg11_bn',\n",
              " 'cifar10_vgg13_bn',\n",
              " 'cifar10_vgg16_bn',\n",
              " 'cifar10_vgg19_bn',\n",
              " 'cifar10_vit_b16',\n",
              " 'cifar10_vit_b32',\n",
              " 'cifar10_vit_h14',\n",
              " 'cifar10_vit_l16',\n",
              " 'cifar10_vit_l32']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "torch.hub.list(\"chenyaofo/pytorch-cifar-models\", force_reload=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2OwEFlMKuws"
      },
      "source": [
        "### Downlaoding the Dataset and Creating the Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-01GV7HHKuws",
        "outputId": "dd4d258c-8fce-47ea-db23-933db0121e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 30164537.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "### Transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "       (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "    )\n",
        "])\n",
        "### Pytorch Datasets\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root= './data', train = True,\n",
        "    download =True, transform = transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root= './data', train = False,\n",
        "    download =True, transform = transform)\n",
        "### Dataloaders\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOZabokbKuwt"
      },
      "source": [
        "### Create the Forget Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-wGG5RC2LW7m"
      },
      "outputs": [],
      "source": [
        "# Define the classes\n",
        "classes = ['forget', 'retain']\n",
        "\n",
        "# Create a dictionary to store datasets for each class\n",
        "class_datasets = {class_name: [] for class_name in classes}\n",
        "\n",
        "# Iterate through the CIFAR-10 dataset and split it into class-specific subsets\n",
        "for image, label in train_dataset:\n",
        "  if label == 1:\n",
        "    class_datasets['forget'].append((image, label))\n",
        "\n",
        "  else:\n",
        "      class_datasets['retain'].append((image, label))\n",
        "\n",
        "# You now have class-specific subsets in the class_datasets dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqTeFK7hKuwu"
      },
      "source": [
        "#### Forget and Retain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RbO79fHuKuwu"
      },
      "outputs": [],
      "source": [
        "# Class split\n",
        "retain_dataloader = torch.utils.data.DataLoader(class_datasets['retain'], batch_size=128, shuffle=True, num_workers=2)\n",
        "forget_dataloader = torch.utils.data.DataLoader(class_datasets['forget'], batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "# Random split\n",
        "# train_split_dataset,forget_split_dataset=torch.utils.data.random_split(train_dataset,lengths=[45000,5000])\n",
        "# retain_dataloader = torch.utils.data.DataLoader(train_split_dataset,  batch_size=128, shuffle=True, num_workers=2)\n",
        "# forget_dataloader = torch.utils.data.DataLoader(forget_split_dataset, batch_size=128, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jfq4c81ePew"
      },
      "source": [
        "# Unlearner class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPWTiJJcePew"
      },
      "source": [
        "## Check that dumb model output random values\n",
        "We expect this values to be uniform in [0,9], thus accuracy to be 10%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HwbSHHzrePew",
        "outputId": "a0e446c2-d150-422b-8282-f194f1d0ea92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from tools.Unlearner_DMM import UnlearnerDMM\n",
        "\n",
        "unlearner = UnlearnerDMM(model)\n",
        "ignorant = unlearner.reset_weights(model)\n",
        "\n",
        "def test(model, dataloader):\n",
        "  tp, n = 0,0\n",
        "  for X,y in dataloader:\n",
        "    X,y = X.to(device), y.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      y_pred = model(X)\n",
        "\n",
        "    tp += (y_pred.argmax(axis=1) == y).sum().item()\n",
        "    n  += y.size(0)\n",
        "\n",
        "  return tp/n\n",
        "\n",
        "test(ignorant, forget_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkwFZ10_ePew"
      },
      "source": [
        "## Run the unlearning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "D4emK6TVePew"
      },
      "outputs": [],
      "source": [
        "unlearner = UnlearnerDMM(model, lr=1e-4, alpha=0.1)\n",
        "unlearner.unlearn(forget_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(unlearner.retained_model, testloader)"
      ],
      "metadata": {
        "id": "jFVcnRJZxdV3",
        "outputId": "ea69eb34-0af8-4e44-b9a9-01fa9e1ce819",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(unlearner.forget_model.parameters()))"
      ],
      "metadata": {
        "id": "g-UukOJJ6p12",
        "outputId": "61c4bc39-58a2-4aa6-cdb6-3b55fd976c68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[ 3.3654e-02, -3.0879e-02, -1.2172e-02],\n",
              "          [ 4.1087e-02, -5.3894e-02, -6.7872e-02],\n",
              "          [ 8.2321e-03,  7.2421e-02, -4.2454e-02]],\n",
              "\n",
              "         [[-7.3649e-02, -9.1706e-05,  6.6639e-02],\n",
              "          [-3.4898e-02,  1.5133e-02,  1.0862e-02],\n",
              "          [ 4.7763e-02, -1.8299e-01, -9.2677e-03]],\n",
              "\n",
              "         [[-4.7221e-02, -5.1088e-02, -3.9762e-02],\n",
              "          [ 3.6060e-02,  1.8792e-02, -9.5315e-02],\n",
              "          [ 1.9576e-02,  1.7739e-02,  1.2735e-03]]],\n",
              "\n",
              "\n",
              "        [[[-1.8645e-02, -3.2253e-02,  6.8319e-03],\n",
              "          [-7.3174e-02, -1.6396e-02, -1.3179e-01],\n",
              "          [-3.5501e-02,  2.5032e-04, -7.8711e-02]],\n",
              "\n",
              "         [[-8.0698e-02, -4.1201e-02, -7.7651e-03],\n",
              "          [ 1.0785e-01, -6.5524e-02, -9.6020e-03],\n",
              "          [-2.8168e-02, -2.8136e-03,  1.8210e-02]],\n",
              "\n",
              "         [[-1.9689e-02, -1.2308e-01,  1.2426e-01],\n",
              "          [-3.8336e-02,  1.3617e-02,  4.9066e-02],\n",
              "          [-3.0487e-02, -1.6265e-02, -9.1628e-02]]],\n",
              "\n",
              "\n",
              "        [[[-5.6400e-02, -3.8077e-02,  6.5238e-02],\n",
              "          [-2.1096e-03,  1.5689e-02, -2.4072e-03],\n",
              "          [ 3.3236e-02, -6.0527e-02,  1.0130e-02]],\n",
              "\n",
              "         [[ 4.1138e-02,  6.9661e-02, -1.1458e-01],\n",
              "          [ 9.2047e-02, -2.0085e-02,  7.6558e-02],\n",
              "          [-4.6708e-02,  1.0063e-01,  4.4672e-02]],\n",
              "\n",
              "         [[ 2.4856e-02,  2.2757e-02, -4.3263e-02],\n",
              "          [-1.6472e-02, -1.2331e-01, -3.8613e-03],\n",
              "          [-4.0095e-03,  8.9688e-02, -1.7831e-02]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[ 1.4853e-02,  3.3558e-02,  2.7853e-02],\n",
              "          [-8.9117e-02, -3.3075e-02,  2.5457e-03],\n",
              "          [-6.1124e-02,  1.0424e-02, -4.3465e-02]],\n",
              "\n",
              "         [[-1.1374e-01, -4.7796e-02, -8.6261e-03],\n",
              "          [ 4.9550e-03, -1.1492e-01, -9.5458e-02],\n",
              "          [ 2.8461e-02, -3.3423e-02,  3.0543e-03]],\n",
              "\n",
              "         [[ 3.5202e-02, -1.3228e-02,  1.1731e-01],\n",
              "          [ 3.1527e-02, -1.9416e-02,  1.4210e-02],\n",
              "          [ 4.9249e-02,  5.1809e-02, -1.8958e-01]]],\n",
              "\n",
              "\n",
              "        [[[ 3.6931e-02,  1.2607e-01, -3.5983e-02],\n",
              "          [-5.1395e-02, -4.6604e-03, -1.2148e-02],\n",
              "          [ 1.1594e-02,  3.7114e-03, -7.8540e-03]],\n",
              "\n",
              "         [[-4.0412e-02,  7.7642e-02, -1.0927e-02],\n",
              "          [-1.3079e-02, -2.0081e-02,  4.1901e-02],\n",
              "          [ 1.5139e-02,  4.8514e-02,  3.7224e-02]],\n",
              "\n",
              "         [[-5.0244e-02, -1.2224e-01,  3.8555e-02],\n",
              "          [-4.1155e-02, -4.5608e-02,  2.3523e-02],\n",
              "          [-3.6986e-03, -6.3649e-02, -1.0383e-01]]],\n",
              "\n",
              "\n",
              "        [[[-1.0562e-04, -7.6715e-03,  7.8343e-02],\n",
              "          [ 2.0661e-02, -1.6226e-01, -3.9507e-02],\n",
              "          [ 2.7931e-02,  3.0239e-02,  1.2240e-01]],\n",
              "\n",
              "         [[ 1.1276e-01, -1.1644e-04,  6.6015e-03],\n",
              "          [ 2.1770e-02,  9.7906e-02,  8.8437e-02],\n",
              "          [-5.5600e-02, -1.1727e-01, -3.4614e-03]],\n",
              "\n",
              "         [[ 4.2727e-02,  7.1223e-02,  3.9884e-02],\n",
              "          [-3.5498e-02,  2.1378e-02, -5.6267e-02],\n",
              "          [ 5.6435e-03,  3.9442e-02,  1.6302e-02]]]], device='cuda:0',\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qx1GPtVePew"
      },
      "source": [
        "We can see that the unlearning process may need many epochs in the erasure phase to converge. We could also try to increase the learning rate for faster convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL-z-m4nePex"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "log_df = pd.DataFrame.from_records(unlearner.log, columns=['phase', 'epoch', 'batch', 'tp', 'n', 'loss'])\n",
        "unlearn_history = log_df.groupby(['phase', 'epoch']).agg({'tp':sum, 'n':sum, 'loss': 'mean'}).reset_index()\n",
        "unlearn_history['accuracy'] = unlearn_history.tp / unlearn_history.n\n",
        "unlearn_history.loc[unlearn_history.phase == 'erasure', 'loss_scale'] = unlearn_history.loc[unlearn_history.phase == 'erasure', 'loss'] /unlearn_history.loc[unlearn_history.phase == 'erasure', 'loss'].max()\n",
        "unlearn_history.loc[unlearn_history.phase == 'retrain', 'loss_scale'] = unlearn_history.loc[unlearn_history.phase == 'retrain', 'loss'] /unlearn_history.loc[unlearn_history.phase == 'retrain', 'loss'].max()\n",
        "\n",
        "sns.lineplot(data = unlearn_history, x = 'epoch', y='loss_scale', hue='phase')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRvbJGBUePex"
      },
      "source": [
        "## Check model performance\n",
        "We now try check the model performance on the retain and forget set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAb2lUjjePex"
      },
      "outputs": [],
      "source": [
        "models = {'original':model, 'dumb':unlearner.dumb_model, 'erased':unlearner.erased_model, 'retrained':unlearner.retrained_model}\n",
        "dataloaders = {'retain':retain_dataloader, 'forget':forget_dataloader, 'test':testloader}\n",
        "\n",
        "performances = []\n",
        "for model_name, m in models.items():\n",
        "  for dl_name, dl in dataloaders.items():\n",
        "    acc = test(m, dl)\n",
        "    performances.append((model_name, dl_name, acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co-ivh1QePex"
      },
      "source": [
        "Strangely the dumb network has 0% performance on the forget set, and this may have a negative impact, note that the performance on the forget set is even too low (we expected 10%, not less).\n",
        "\n",
        "Outside of that we note that this procedure works great for the retain and test which are fitted very nicely, and we can even see that there's a big leap in performance from the erased model to the retrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BH10Cc8YePex"
      },
      "outputs": [],
      "source": [
        "perf_df = pd.DataFrame.from_records(performances, columns=['model', 'data_partition', 'accuracy'])\n",
        "\n",
        "tb = pd.pivot_table(perf_df, index='data_partition', columns='model', values='accuracy')\n",
        "sns.heatmap(tb, annot = True, fmt='.2%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}