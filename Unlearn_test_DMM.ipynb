{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/nem-42098/SL_Proj_Unlearning/blob/main/Unlearn_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCIXLmbpKuwo",
        "outputId": "f0452157-f739-4f42-c2c7-5d248ac5facc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'SL_Proj_Unlearning'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 31 (delta 12), reused 24 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (31/31), 15.09 KiB | 2.51 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/nem-42098/SL_Proj_Unlearning.git\n",
        "import os\n",
        "os.chdir('/content/SL_Proj_Unlearning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6CbLaIs8Kuwq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyJgdsOfKuwr"
      },
      "source": [
        "### Load Pre-Trained VGG network\n",
        "> #### https://github.com/chenyaofo\n",
        "> ### Note: There is some issue with using Batch Norm before ReLu as it creates a bias in the network. So people exchange the order between the two for tackling the bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wvUMz6DKuwr",
        "outputId": "8b291d56-9a33-417f-a4ef-0d43d3bb4dbc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/chenyaofo/pytorch-cifar-models/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "Downloading: \"https://github.com/chenyaofo/pytorch-cifar-models/releases/download/vgg/cifar10_vgg11_bn-eaeebf42.pt\" to /root/.cache/torch/hub/checkpoints/cifar10_vgg11_bn-eaeebf42.pt\n",
            "100%|██████████| 37.3M/37.3M [00:00<00:00, 161MB/s]\n"
          ]
        }
      ],
      "source": [
        "### First time when you wan to download the model\n",
        "device=torch.device('cuda')\n",
        "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_vgg11_bn\", pretrained=True)\n",
        "model=model.to(device)\n",
        "### For future uses:Loading from the local\n",
        "\n",
        "# model_1=torch.hub.load(\"C:/Users/nmura/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\",'hubconf.py',source='local')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqWzDJ7YKuwr"
      },
      "source": [
        "### Check which pre-trained model are available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSxUWBRyKuwr",
        "outputId": "22605340-a9f4-46cf-fdc0-71e03ffda65b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/chenyaofo/pytorch-cifar-models/zipball/master\" to /home/mamiglia/.cache/torch/hub/master.zip\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['cifar100_mobilenetv2_x0_5',\n",
              " 'cifar100_mobilenetv2_x0_75',\n",
              " 'cifar100_mobilenetv2_x1_0',\n",
              " 'cifar100_mobilenetv2_x1_4',\n",
              " 'cifar100_repvgg_a0',\n",
              " 'cifar100_repvgg_a1',\n",
              " 'cifar100_repvgg_a2',\n",
              " 'cifar100_resnet20',\n",
              " 'cifar100_resnet32',\n",
              " 'cifar100_resnet44',\n",
              " 'cifar100_resnet56',\n",
              " 'cifar100_shufflenetv2_x0_5',\n",
              " 'cifar100_shufflenetv2_x1_0',\n",
              " 'cifar100_shufflenetv2_x1_5',\n",
              " 'cifar100_shufflenetv2_x2_0',\n",
              " 'cifar100_vgg11_bn',\n",
              " 'cifar100_vgg13_bn',\n",
              " 'cifar100_vgg16_bn',\n",
              " 'cifar100_vgg19_bn',\n",
              " 'cifar100_vit_b16',\n",
              " 'cifar100_vit_b32',\n",
              " 'cifar100_vit_h14',\n",
              " 'cifar100_vit_l16',\n",
              " 'cifar100_vit_l32',\n",
              " 'cifar10_mobilenetv2_x0_5',\n",
              " 'cifar10_mobilenetv2_x0_75',\n",
              " 'cifar10_mobilenetv2_x1_0',\n",
              " 'cifar10_mobilenetv2_x1_4',\n",
              " 'cifar10_repvgg_a0',\n",
              " 'cifar10_repvgg_a1',\n",
              " 'cifar10_repvgg_a2',\n",
              " 'cifar10_resnet20',\n",
              " 'cifar10_resnet32',\n",
              " 'cifar10_resnet44',\n",
              " 'cifar10_resnet56',\n",
              " 'cifar10_shufflenetv2_x0_5',\n",
              " 'cifar10_shufflenetv2_x1_0',\n",
              " 'cifar10_shufflenetv2_x1_5',\n",
              " 'cifar10_shufflenetv2_x2_0',\n",
              " 'cifar10_vgg11_bn',\n",
              " 'cifar10_vgg13_bn',\n",
              " 'cifar10_vgg16_bn',\n",
              " 'cifar10_vgg19_bn',\n",
              " 'cifar10_vit_b16',\n",
              " 'cifar10_vit_b32',\n",
              " 'cifar10_vit_h14',\n",
              " 'cifar10_vit_l16',\n",
              " 'cifar10_vit_l32']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.hub.list(\"chenyaofo/pytorch-cifar-models\", force_reload=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2OwEFlMKuws"
      },
      "source": [
        "### Downlaoding the Dataset and Creating the Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-01GV7HHKuws",
        "outputId": "428b3ea3-bcb0-4e87-d2aa-dfe2ab381588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 74589105.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "### Transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "       (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "    )\n",
        "])\n",
        "### Pytorch Datasets\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root= './data', train = True,\n",
        "    download =True, transform = transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root= './data', train = False,\n",
        "    download =True, transform = transform)\n",
        "### Dataloaders\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cclKNSe-Kuws",
        "outputId": "94d263ec-838a-45e8-c338-9f298847ac15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOZabokbKuwt"
      },
      "source": [
        "### Create the Forget Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-wGG5RC2LW7m"
      },
      "outputs": [],
      "source": [
        "# Define the classes\n",
        "classes = ['forget', 'retain']\n",
        "\n",
        "# Create a dictionary to store datasets for each class\n",
        "class_datasets = {class_name: [] for class_name in classes}\n",
        "\n",
        "# Iterate through the CIFAR-10 dataset and split it into class-specific subsets\n",
        "for image, label in train_dataset:\n",
        "  if label == 1:\n",
        "    class_datasets['forget'].append((image, label))\n",
        "\n",
        "  else:\n",
        "      class_datasets['retain'].append((image, label))\n",
        "\n",
        "# You now have class-specific subsets in the class_datasets dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqTeFK7hKuwu"
      },
      "source": [
        "#### Forget and Retain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "RbO79fHuKuwu"
      },
      "outputs": [],
      "source": [
        "# Class split\n",
        "retain_dataloader = torch.utils.data.DataLoader(class_datasets['retain'], batch_size=128, shuffle=True, num_workers=2)\n",
        "forget_dataloader = torch.utils.data.DataLoader(class_datasets['forget'], batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "# Random split\n",
        "# train_split_dataset,forget_split_dataset=torch.utils.data.random_split(train_dataset,lengths=[45000,5000])\n",
        "# retain_dataloader = torch.utils.data.DataLoader(train_split_dataset,  batch_size=128, shuffle=True, num_workers=2)\n",
        "# forget_dataloader = torch.utils.data.DataLoader(forget_split_dataset, batch_size=128, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unlearner class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check that dumb model output random values\n",
        "We expect this values to be uniform in [0,9], thus accuracy to be 10%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tools.Unlearner import Unlearner\n",
        "\n",
        "unlearner = Unlearner(model, lr = 1e-6, alpha=1)\n",
        "ignorant = unlearner.reset_weights(model)\n",
        "\n",
        "def test(model, dataloader):\n",
        "  tp, n = 0,0\n",
        "  for X,y in dataloader:\n",
        "    X,y = X.to(device), y.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      y_pred = model(X)\n",
        "\n",
        "    tp += (y_pred.argmax(axis=1) == y).sum().item()\n",
        "    n  += y.size(0)\n",
        "\n",
        "  return tp/n\n",
        "\n",
        "test(ignorant, forget_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the unlearning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tools.Unlearner import Unlearner\n",
        "\n",
        "unlearner = Unlearner(model, lr = 1e-6, alpha=1)\n",
        "\n",
        "unlearn_model = unlearner.unlearn(retain_dataloader, forget_dataloader, forget_epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the unlearning process may need many epochs in the erasure phase to converge. We could also try to increase the learning rate for faster convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "log_df = pd.DataFrame.from_records(unlearner.log, columns=['phase', 'epoch', 'batch', 'tp', 'n', 'loss'])\n",
        "unlearn_history = log_df.groupby(['phase', 'epoch']).agg({'tp':sum, 'n':sum, 'loss': 'mean'}).reset_index()\n",
        "unlearn_history['accuracy'] = unlearn_history.tp / unlearn_history.n\n",
        "unlearn_history.loc[unlearn_history.phase == 'erasure', 'loss_scale'] = unlearn_history.loc[unlearn_history.phase == 'erasure', 'loss'] /unlearn_history.loc[unlearn_history.phase == 'erasure', 'loss'].max() \n",
        "unlearn_history.loc[unlearn_history.phase == 'retrain', 'loss_scale'] = unlearn_history.loc[unlearn_history.phase == 'retrain', 'loss'] /unlearn_history.loc[unlearn_history.phase == 'retrain', 'loss'].max() \n",
        "\n",
        "sns.lineplot(data = unlearn_history, x = 'epoch', y='loss_scale', hue='phase')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check model performance\n",
        "We now try check the model performance on the retain and forget set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {'original':model, 'dumb':unlearner.dumb_model, 'erased':unlearner.erased_model, 'retrained':unlearner.retrained_model}\n",
        "dataloaders = {'retain':retain_dataloader, 'forget':forget_dataloader, 'test':testloader}\n",
        "\n",
        "performances = []\n",
        "for model_name, m in models.items():\n",
        "  for dl_name, dl in dataloaders.items():\n",
        "    acc = test(m, dl)\n",
        "    performances.append((model_name, dl_name, acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Strangely the dumb network has 0% performance on the forget set, and this may have a negative impact, note that the performance on the forget set is even too low (we expected 10%, not less).\n",
        "\n",
        "Outside of that we note that this procedure works great for the retain and test which are fitted very nicely, and we can even see that there's a big leap in performance from the erased model to the retrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "perf_df = pd.DataFrame.from_records(performances, columns=['model', 'data_partition', 'accuracy'])\n",
        "\n",
        "tb = pd.pivot_table(perf_df, index='data_partition', columns='model', values='accuracy')\n",
        "sns.heatmap(tb, annot = True, fmt='.2%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
